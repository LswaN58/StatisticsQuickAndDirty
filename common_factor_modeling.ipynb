{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Factor Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis gives us a bunch of equations of form:  \n",
    "$X_j = \\mu_j + \\lambda_{j1} f_1  + \\lambda_{j2} f_2 + \\ldots + \\lambda_{jm} f_m + e_j$\n",
    "\n",
    "Now, we can subtract out the $\\mu_j$ to get $X'_j = \\lambda_{j1} f_1  + \\lambda_{j2} f_2 + \\ldots + \\lambda_{jm} f_m + e_j$.\n",
    "Then we can write the $X'_j$ as a matrix, I think.\n",
    "\n",
    "And then there's some steps I've missed, but eventually we can write our variance matrix as $\\sum_{xx} = \\Lambda \\Lambda' + \\Psi$, where the $\\Lambda$ matrix is somehow derived from our matrix of $\\lambda$ variables.\n",
    "Maybe it's just that directly?\n",
    "And then $\\Psi$ is somehow related to the error terms, I think. It's definitely stated to be a diagonal matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations of CFMs\n",
    "\n",
    "So, consider an orthogonal matrix $M$.\n",
    "By definition of **orthogonal matrix**, $M M' = I$.  \n",
    "Then we can write $\\sum_{xx} = \\Lambda M M' \\Lambda' + \\Psi$\n",
    "Note that we can then interpret $\\Lambda M$ as a new matrix $L$, and $M' \\Lambda$ as $L'$.\n",
    "This shows us that such an operation gives us an equivalent CFM, from the perspective of explained variance, but with the vectors all transformed.\n",
    "\n",
    "Generally, rotation matrices are orthogonal.  \n",
    "Then we can essentially use rotation matrices to manipulate the vectors in $\\Lambda$ until we find a set that gives us the nicest interpretation.\n",
    "\n",
    "What does it mean to get a \"nice\" interpretation?\n",
    "Consider a two-factor model.\n",
    "We can create a 2D graph of each variable, where for $X_j$, we have $\\lambda_{j1}$ on the x-axis and $\\lambda_{j2}$ on the y-axis.\n",
    "Then we can project each variable onto the x and y axes (really, just taking the x- and y-values), to see how strongly related they are to each factor.\n",
    "\n",
    "If each variable is moderately related to both factors, it's hard to say what the factors are.\n",
    "It is easier to understand each factor if it is strongly related to some variables, and not at all to others.\n",
    "Thus, a rotation for our CFM is effectively a rotation of the axis, and we rotate the axes until our variables are (ideally) all close to one axis and not the other.\n",
    "\n",
    "Types of CFM rotations:\n",
    "\n",
    "1. Varimax: A rigid rotation, where the axes remain orthogonal.\n",
    "    This approach finds the maximum variance of $\\lambda$'s across factors (not quite sure what that means, need to think about it a bit more)\n",
    "2. Promax: An oblique rotation (rotation with skew, I think), such that the axes are not necessarily orthogonal.  \n",
    "    In this case, the factors become correlated.\n",
    "3. Others exist, not discussed yet..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oblique Rotations\n",
    "\n",
    "I think this is the rotation(s) that don't maintain orthogonal axes.\n",
    "\n",
    "In the past, we always talked about loadings and correlations as one and the same.\n",
    "With oblique rotation, a variable can now be correlated to a factor, without the factor being loaded on the variable.\n",
    "\n",
    "Also, note that there's not really a single goodness of fit measure for the promax/oblique approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matrices\n",
    "\n",
    "This gives the factor loadings on each variable.\n",
    "\n",
    "This is generally supposed to give even \"better\" results than a rotation, I guess, in terms of interpretability? Or something..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Matrices\n",
    "\n",
    "This gives correlations between variables and factors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrices\n",
    "\n",
    "We also get a factor correlation matrix, which shows correlation between the two factors in... our pattern matrix? I think?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Matrix\n",
    "\n",
    "We can look at matrix that gives differences (residuals) between original correlations between variables, and the correlations after transform, or something.\n",
    "idfk.\n",
    "However the fuck it's calculated, this matrix tells us how much over- or under-estimation there is of correlation between two variables, or the correlation between the two that isn't accounted for by the factor model.\n",
    "\n",
    "We can get somewhat of a single value by doing root mean square of residuals (RMSr index), i.e. sum the squared residuals, divide by number of residuals, and take square root, then we get something approximating an average residual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
