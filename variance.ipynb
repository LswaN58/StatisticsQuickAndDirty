{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of variance as a sort of Venn diagram, where each variable has some variance, represented by the variables' individual circles.\n",
    "Then the overlap is the correlation between the two variables, and the size is the total amount of variance.\n",
    "All are subsets of the overall variation.\n",
    "\n",
    "In the case of PCA, we are reorganizing our data such that each component has as large a size as possible, has no overlap with other components, and is sorted in descending size order.\n",
    "Also, we could imagine overlaying this over the original diagram (although this might require our components to be given in pieces), and the overlap between components and variables is the weighting of the variables in the component."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there was something about the difference between linear regression and PCA that I didn't quite catch.\n",
    "I suppose regression is finding line that minimizes distance to line when projecting points, and PCA is finding line that maximizes variance after projecting points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs. Covariance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is heavily biased toward the variables with high variance, I guess... really got to spend more time thinking about this.\n",
    "If we do PCA against a covariance matrix with one high-variance variable, we will get a component that just has maximum weight on that variable, and has a super-high eigenvalue.\n",
    "\n",
    "So, we only want to do covariance matrix in PCA if the variances/variables are reasonably comparable.\n",
    "This does not necessarily come from just having the same units, either."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use each?\n",
    "\n",
    "1. Correlation Matrix:  \n",
    "    General cases where some variables have higher or lower scales variances, and we don't want to overlook lower-variance variables.\n",
    "2. Covariance Matrix:  \n",
    "    Cases where all variables have similar scale of variance.\n",
    "    For example, a questionnaire where we know everything is on a common Likert scale.\n",
    "    Also, it's nice in general for questionnaires because we probably want to bias towards components that use high-variance questions, since those are giving the most information on the subjects.\n",
    "    Correlation matrix would treat all questions equally-ish."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we take a PCA with covariance matrix.\n",
    "We can standardize our raw component scores, basically divide each component's raw component scores by standard deviation of the individual components, or something.\n",
    "And then if we further divide each variable score in a component by the variable standard deviation, we would be left with correlations between the variables and their components.\n",
    "\n",
    "Then the interpretation is the same as you'd have if you did a correlation matrix PCA from the beginning.\n",
    "The values themselves will be different, though, I guess.\n",
    "On account of doing the spectral decomposition on a different matrix, I assume."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
