{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Factor Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Methods\n",
    "\n",
    "Recall, we may have factor analysis cases where the problem is overspecified.\n",
    "In fact, this is probably the most common case.\n",
    "\n",
    "When we face one of these, we want to choose the parameter estimates that are \"closest\" fit to the data.\n",
    "There's different ways to determine what we consider \"close\" fits for the data, in terms of choosing $\\lambda$'s and $\\psi$'s.  \n",
    "\n",
    "1. Unweighted Least Squares (ULS):  \n",
    "    Just like in regression, we look for solution that minimizes squared diferences between observed and predicted.\n",
    "    This is the simplest, most intuitive approach, but has no test for significance to determine if the residuals are \"small enough\" to call the model \"good.\"\n",
    "2. Generalized Least Squres (GLS):  \n",
    "    This is like least squares, but weights items in the residual matrix.\n",
    "    For large samples, we can do a $\\Chi^2$ test for goodness of fit, and some other advantages I fucking missed, and some disadvantages I fucking missed.\n",
    "3. Maximum Likelihood (ML):  \n",
    "    Uses some assumptions about population, not quite sure what those are.\n",
    "    Iteratively search parameter space ($\\lambda$ and $\\psi$ values) for the set of parameters that make the observed (training) data most likely.\n",
    "    This has advantage of including a statistical test for model fit, and gives more accurate parameter estimates if our sample size is large.\n",
    "    Additionally, if assumptions hold, it gives best estimate of correlation matrix, or something.\n",
    "    But computationally intensive, and can apparently give you nonsensical values for params (e.g. negative values for params that should only be $\\ge 0$).\n",
    "4. Principal Axis Factoring (PAF):  \n",
    "    Here, we try to estimate the communalities of the factors, I guess.\n",
    "    This approach is often called Principal Component Factor Analysis.\n",
    "    Details in section below.\n",
    "5. Alpha factoring:\n",
    "    Not discussed in class.\n",
    "    Kind of a holdover from days when computers needed less computationally-intensive approaches.\n",
    "6. Image factoring:\n",
    "    Not discussed in class.\n",
    "    Kind of a holdover from days when computers needed less computationally-intensive approaches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Axis Factoring\n",
    "\n",
    "Recall, PCA was based on a spectral decomposition.\n",
    "We had idea of communality, which is the shared variance, and the uniqueness, which is unique variance.  \n",
    "In PAF, instead of decomposing the correlation/covariance matrix directly, we'll take the correlation matrix, put communalities on the diagonal, and do our decomposition on that.\n",
    "\n",
    "The trouble is, we don't really know communalities/uniqueness, until we know our factors.\n",
    "But we don't know our factors until we do our Principal Axis Factoring.\n",
    "Thus, we will get an initial estimate, and iterate.  \n",
    "Now, communality is variance that is shared by common factors, so typically we take the $R^2$ for each variable as our initial communalities, since it's a measure of shared variance.\n",
    "\n",
    "After decomposing that, we can do some math and start next iteration.\n",
    "Specifically, we take the decomposition, then take the first however-many columns for the number of factors we want.\n",
    "We take each column, multiply by the square root of the eigenvalue, and have a loading.\n",
    "Then, if we... I think square that? Or something? Whatever math, we do the step to get the communality.\n",
    "We then use these to update the diagonal of our matrix, and iterate.\n",
    "\n",
    "Eventually, we hope for it to converge, and then we can take our first $n$ columns as our $n$ factors.\n",
    "Not stated if there are any proofs of convergence for certain cases.\n",
    "\n",
    "Advantage to this method is a connection to PCA, and... yeah.\n",
    "A couple disadvantages are: no measure of goodness of fit, and doesn't generally have nice statistical properties to results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a method:\n",
    "\n",
    "There's a few things to consider:\n",
    "1. Sample size:\n",
    "    GLS only has significance test with large sample, for example.\n",
    "    For small samples, that most methods don't have a test.\n",
    "2. Number of variables:\n",
    "    As number of variables increases, different methods' results tend to become more similar.\n",
    "3. Magnitude of communalities:\n",
    "    If all are close to 1, then methods will all give similar results, doesn't really matter which you used.\n",
    "4. Variability of communalities:\n",
    "    When all variables have similar communalities (near 1 or otherwise), PAF and PCA will give very similar results, for example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heywood Cases\n",
    "\n",
    "Case where a communality goes above 1, or a uniqueness below 0.\n",
    "Such cases don't give a meaningful result, basically the solution is nonsensical wrt statistics.\n",
    "Both communality and uniqueness are proportions of variance, and we can't have proportions of a thing go outside the 0-1 range."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
