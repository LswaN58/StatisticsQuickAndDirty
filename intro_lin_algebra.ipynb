{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Linear Algebra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Math\n",
    "\n",
    "We call two vectors **orthogonal** when their inner (dot) product is 0.\n",
    "That is, $\\textbf{a} \\cdot \\textbf{b} = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Math\n",
    "\n",
    "The **identity matrix** $I$ has 1's on the diagonal, and 0's elsewhere.\n",
    "For any matrix $A$, $A \\cdot I = A$.\n",
    "\n",
    "Some matrices $A$ have an **inverse** matrix $A^{-1}$, such that $A*A^{-1} = I$.\n",
    "Such matrices are called **singular** matrices.\n",
    "\n",
    "The **determinant** of a matrix is... somthing, I forget the calculation.\n",
    "A **singular** matrix has non-zero determinant.\n",
    "A **non-singular** matrix does not have full rank; that is, there are at least two columns that are... parallel, I want to say?\n",
    "\n",
    "The **transpose** of a matrix $A$ is notated as $A^T$, and swaps the rows and columns.\n",
    "A **symmetric** matrix has $A = A^T$.\n",
    "A symmetric matrix is called positive definite if $\\forall \\textbf{x}$, if $\\textbf{x}$ is non-zero, then $x'Ax' \\ge 0$.\n",
    "Put another way, a **symmetric positive definite (SPD) matrix** does not take any nonzero vector to 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**covariance** between two variables in a matrix:  \n",
    "$Cov(j, k) = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_{ij} - \\mu_j)(x_{ik} - \\mu_k)$, where $\\mu_j, \\mu_k$ are the column means of columns $j, k$.\n",
    "\n",
    "So, this is effectively an inner product of columns $j, k$, where we've first subtracted the mean from each element, divided by number of rows.\n",
    "So could be stated as $Cov(j, k) = \\frac{1}{n}(\\textbf{j} - \\mu_j) \\cdot (\\textbf{k} - \\mu_k)$\n",
    "Often label $Cov(j, k)$ as $\\sigma_{jk}$\n",
    "\n",
    "We often refer to subtraction of mean from a vector as yielding **mean-corrected** vectors.\n",
    "\n",
    "A high positive covariance indicates the variables are highly similar, as a high positive dot product would generally indicate.\n",
    "High negative covariance would indicate inverse relationship.\n",
    "\n",
    "Variance of a single variable is just $Cov(j, j)$, or $\\sigma_j^2$\n",
    "\n",
    "Variance is interpreted as we have previously learned. A variance of 0 indicates a constant value; a high variance indicates a wide spread of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **correlation** between two variables in a matrix:  \n",
    "$Cor(j, k) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{x_{ij} - \\mu_j}{\\sigma_j} \\frac{x_{ik} - \\mu_k}{\\sigma_k}$\n",
    "\n",
    "Often label $Cor(j, k) = \\rho_{jk}$\n",
    "\n",
    "Similar to covariance, we are taking an inner product, divided by number of rows.\n",
    "In this case, we have **standardized** or **z-transformed** vectors, which have their means subtracted and divided by standard deviation.\n",
    "\n",
    "Note, the square of correlation is the proportion of variance shared between the variables.\n",
    "\n",
    "Also, note $\\rho_{jk} = \\frac{\\sigma_{jk}}{\\sigma_j \\sigma_k}$.\n",
    "Then if we have the standard deviations of the variables $j$ and $k$, we can relate them with the correlation.\n",
    "Now, the use of Greek letters implies the general, population parameter values.  \n",
    "When we have a sample, and generate the statistics for the sample, we'll use Roman letters.\n",
    "For this, then, we have $r_{jk} = \\frac{s_{xy}}{s_x s_y}$\n",
    "\n",
    "For any constant $c$, and variables $i, j, k$, we have:\n",
    "- $Cov(c, \\textbf{i}) = 0$\n",
    "- $Cov(c\\textbf{i}, \\textbf{j}) = c \\cdot Cov(\\textbf{i}, \\textbf{j})$\n",
    "- $Cov(\\textbf{i}, \\textbf{k}) + Cov(\\textbf{j}, \\textbf{k}) = Cov(\\textbf{i} + \\textbf{j}, \\textbf{k})$\n",
    "\n",
    "If we attempted to look at equivalences for correlations:\n",
    "- TODO\n",
    "- $Cor(c\\textbf{i}, \\textbf{j}) = Cor(\\textbf{i}, \\textbf{j})$ (Because we standardize, scaling **i** doesn't change its correlation)\n",
    "- TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent all the covariances by a covariance matrix, where element $i, j$ is the covariance between columns $i$ and $j$.\n",
    "This can be computed by: ... missed it, get from notes\n",
    "\n",
    "Note that the covariance matrix will be symmetric, because $Cov(i, j) = Cov(j, i)$.\n",
    "\n",
    "We can do the same with a **correlation matrix**.\n",
    "The **correlation matrix** is symmetric, 100% for sure.\n",
    "\n",
    "**covariance** and **correlation** matrices are SPD matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
