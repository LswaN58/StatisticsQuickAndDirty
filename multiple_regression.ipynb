{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize regression to include multiple explanatory variables.\n",
    "This may be helpful to build a better model, or to account for confounding variables and allow better isolation of our analysis of a relationship between a specific explanatory variable to the outcome.\n",
    "\n",
    "General form is $\\hat{y} = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, apparently bivariate regression models define a best-fit _plane_, rather than 3D line, which was my assumption. Need to think a bit more about that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression and Spurious Relations\n",
    "\n",
    "Consider a univariate regression model for a statistically significant spurious relationship.\n",
    "Such a relation will result in a model with significant slope.\n",
    "\n",
    "If we introduce another explanatory variable and create a bivariate model, where the new variable has a true causative effect, then the original explanatory variable will no longer be a significant part of the model. Apparently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "For multiple regression, we can use squared partial correlations.\n",
    "Need to look back at notes to get how to do it.\n",
    "\n",
    "We generate a correlation matrix to see how closely each variable is related to the others.\n",
    "Ideally, all correlations between explanatory variables will be **statistically significant**, but small.  \n",
    "In general, $r >= 0.8$ is used as a threshold for dropping one from a pair of explanatory variables.\n",
    "\n",
    "We can also calculate $R^2$, the overall proportion of explained variance.\n",
    "Called the **coefficient of multiple determination**, or something like that.  \n",
    "\n",
    "Taking square root gives us $R$, the **multiple correlation**.\n",
    "This is giving us correlation between observed $y$ and predicted $\\hat{y}$.\n",
    "$R$ is always between 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "We can interpret individual slopes by saying \"when adjusting for \\[other variables\\], \\[outcome\\] changes by \\[slope\\] with a unit increase in \\[explanatory variable\\].\"  \n",
    "Could also say \"when holding \\[other variables\\] constant, ...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Regression Equations\n",
    "\n",
    "We can get a \"partial\" equation for a specific explanatory variable by substituting specific values for all other explanatory variables.\n",
    "\n",
    "This gives us the equation under the conditions created by those specific values for the other variables.\n",
    "We'll effectively get a new intercept, which is our base for those conditions, when the specific explanatory variable of interest is 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
