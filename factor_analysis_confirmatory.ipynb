{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirmatory Factor Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A while ago, people used techniques we would now consider exploratory in a way we would now consider confirmatory.\n",
    "This is now frowned upon, as new confirmatory-specific techniques have been developed.\n",
    "\n",
    "Basically, we have exploratory approaches as discussed in basic \"Factor Analysis\" and the \"Exploratory Factor Analysis\" sections.\n",
    "These are meant to help us find the factors to use.  \n",
    "We will now cover some confirmatory approaches, which are used to determine if the factors we identified previously do, in fact, measure what we think they're measuring.\n",
    "\n",
    "Note, exploratory should probably be skipped if we already have a set of factors in mind, e.g. if we're developing an instrument to measure a few specific constructs.\n",
    "In that case, we should go directly to confirmatory factor analysis, which will let us say \"there should be X number of factors, but when we try to confirm that, X seems wrong, and here's why.\"\n",
    "The exploratory approach would be appropriate in cases where we're not sure how many factors should be present, and we want to find out what factors exist.\n",
    "\n",
    "Confirmatory analysis, generally, is a tool for:\n",
    "1. Estimating $\\lambda$ and $\\psi$ parameters for a proposed FA model\n",
    "2. Evaluating fit of given factor model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs. Covariance\n",
    "\n",
    "In exploratory factor analysis, we generally use correlation matrices.\n",
    "For confirmatory, we generally prefer covariance matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uh... general format and math-y stuff?\n",
    "\n",
    "Recall, we had a version of decomposition before where we used:  \n",
    "$R = \\Lambda \\Phi \\Lambda' + \\Psi$  \n",
    "There, $\\Phi$ was some correlation matrix, and $\\Psi$ was diagonal, I think.\n",
    "Now, we have something similar in confirmatory.\n",
    "Any confirmatory analysis is going to try to say something about the $\\Lambda$, $\\Phi$ and $\\Psi$ matrices.\n",
    "\n",
    "Now, $\\Lambda$ would have one column for each factor, and one row for each measured variable.\n",
    "Ideally, each column has a few \"real\" $\\lambda$ values that are well above 0, for the highly-loaded variables, and the rest very close to 0.  \n",
    "$\\Phi$ has 1's on the diagonal, and correlation values $\\phi$ in all the off-diagonals, I think.  \n",
    "Finally, $\\Psi$ has individual $\\psi$ on the diagonal, missed what those are, but we typically have 0's on off-diagonal spaces because we didn't want any correlations between... whatever $\\psi$'s are. Uniquenesses, maybe?\n",
    "Anyway, I guess we relax this for the confirmatory analysis.\n",
    "So, previously we needed the 0's as an assumption that let us get certain constraints on $\\Lambda$ and $\\Phi$, or some shit.\n",
    "Not totally clear on that.\n",
    "Whatever the reasoning, in confirmatory analysis, we will start with assumption $\\Psi$ is a diagonal matrix, but relax the assumption and let off-diagonals become non-0.\n",
    "\n",
    "We can start by fitting a model that forces all $\\lambda$ to be equal, with a one-factor model.\n",
    "Then, we can use that to compare with other models, to determine if they are better than that highly-constrained model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary,\n",
    "- $\\Lambda$ is the factor loading matrix,\n",
    "- $\\Psi$ is a diagonal matrix of correlations... or uniquenesses... or something\n",
    "- $\\Phi$ is also diagonal, and the values are... I dunno."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches by identification problem case\n",
    "\n",
    "1. Underidentified Model:  \n",
    "    Consider one-factor model on two measured variables.\n",
    "    We'd have 5 parameters to estimate, with only 3 unique values in our 2x2 correlation/covariance matrix.\n",
    "    Um... didn't really say what to do, other than to note in EFA case we'd assume variance value of 1 for $\\psi_{11}$, or something?\n",
    "2. Just-identified Model:  \n",
    "    Can't really find a meaningful measure of fit here.\n",
    "3. Over-identified Model:  \n",
    "    Best condition to have.\n",
    "    There's more covariance/correlation values than params to estimate, so we can get a measure of fit in addition to param estimates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification Rules\n",
    "\n",
    "There are rules for necessary and sufficient conditions for identifiability.\n",
    "There is no single rule that is both necessary and sufficient, so I guess we need some combinations to get identifiable case?\n",
    "Not even really clear what's meant by identifiability here, like if that means we fit into case 2 or 3 above; or if it's for us to be in that case *and* be mathematically good for analysis?\n",
    "\n",
    "Anyway, here's some rules:\n",
    "1. t-rule:  \n",
    "    Number of free params (t) must be $\\le$ number of unique elements in covariance matrix.\n",
    "    For a $p \\times p$ matrix, we have $\\frac{p(p+1)}{2}$ parameters, and... missed calculation for how many unique elements, but we must have $t \\le$ that number to have an identified problem.\n",
    "    Anyway, this rule is necessary but not sufficient.\n",
    "    We can meet this rule, but still have a model that is not identified.\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
