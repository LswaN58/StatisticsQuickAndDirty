{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike PCA, factor analysis attempts to explain only the shared variance between variables.\n",
    "\n",
    "Further, the number of factors you use changes the values associated with the factors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis builds a model of similar form to a regression.\n",
    "That is, a single-factor factor analysis has the same interpretation as a regression model.\n",
    "It has the form:  \n",
    "$X_j = \\mu_j \\lambda_j f + e_j$,  \n",
    "where $X_j$ is the outcome variable predicted by a constant $\\mu_j$ and slope $\\lambda_j$ for a linear relationship with variable $f$.\n",
    "Finally, there is an error term $e_j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general factor analysis is similar to a multiple regression, with $m$ factors $f_i$.\n",
    "Then for each observed variable $X_j$, we have a model of the form:  \n",
    "$X_j = \\mu_j + \\lambda_{j1} f_1  + \\lambda_{j2} f_2 + \\ldots + \\lambda_{jm} f_m + e_j$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, unlike a true regression analysis, we don't have direct observation of the factors $f_i$.\n",
    "We make the assumption that such a model form describes each outcome variable, and then try to estimate the $\\lambda_{ji}$.\n",
    "We'll refer to these \"regression slopes\" as \"factor loadings.\"\n",
    "\n",
    "So, this leads us to only attempt a factor analysis if we have multiple outcome variables $X_j$, I believe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simplifying assumptions\n",
    "\n",
    "We can make some simplifying assumptions for our latent factors $f_i$.\n",
    "Because, they're \"latent,\" not a real metric, we can safely make these assumptions.\n",
    "Really, we're putting constraints on our solution, I think.\n",
    "\n",
    "1. Assume all $f_i$ have mean 0.\n",
    "2. Assume all $f_i$ have standard deviation of 1.\n",
    "\n",
    "From these two, we basically are assuming $f_i$ are standardized, or z-scored.\n",
    "\n",
    "3. Missed\n",
    "4. Missed\n",
    "5. $\\forall j, k: Cov(e_j, f_k) = 0$.\n",
    "    That is, factors are non-covariate with random errors.\n",
    "6. $\\forall f_k, f_{k'}: Cov(f_k, f_{k'})$.\n",
    "    That is, factors are pairwise non-covariate.\n",
    "    Advanced techniques relax this assumption.\n",
    "7. $\\forall e_j, e_{j'}: Cov(e_j, e_{j'}) = 0$.\n",
    "    That is, random error for variables are pairwise non-covariate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the assumptions, we can use some algebra on variances and covariances to establish a relationship between population covariance matrix $\\Sigma_{xx}$ and the parameters on factors $f_i$ (that is, the $\\lambda_{ji}$ values)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing stuff\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-factor case\n",
    "\n",
    "Recall, covariance of a constant with a variable is always 0, since constant has $\\sigma$ of 0.\n",
    "Also, variance of a variable plus a constant is just the variance of the variable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proving $Var(X_j) = \\lambda_j^2 + \\psi_j$\n",
    "\n",
    "Now, we have $Var(X_j) = Var(\\mu_j + \\lambda_j f + e_j)$, since $X_j$ is assumed to be modeled by $\\mu_j + \\lambda_j f + e_j$.  \n",
    "$\\mu_j$ is constant, so $Var(\\mu_j + \\lambda_j f + e_j) = Var(\\lambda_j f + e_j)$  \n",
    "Further, since $Var(x + y) = Var(x) + Var(y) + 2Cov(x, y)$, we have $Var(\\lambda_j f + e_j) = Var(\\lambda_j f) + Var(e_j) + 2 Cov(\\lambda_j f, e_j)$.  \n",
    "But $Cov(\\lambda_j f, e_j) = 0$ by assumption, so we have $Var(\\lambda_j f) + Var(e_j) + 2 Cov(\\lambda_j f, e_j) = Var(\\lambda_j f) + Var(e_j)$.  \n",
    "We can factor the first term to get $\\lambda_j^2 Var(f) + Var(e_j)$.\n",
    "Finally, we have $Var(f) = 1$ by assumption, and we notate the variance of error by $\\psi_j = Var(e_j)$.  \n",
    "$\\therefore Var(X_j) = \\lambda_j^2 + \\psi_j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the proof above shows that under our assumptions/constraints for the factors $f$, the variance of our observed outcome variable is the sum of squares of slopes (single slope for single-factor case), plus some error variance unexplained by the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proving Cov(X_i, X_j) = \\lambda_i \\lambda_j\n",
    "\n",
    "By assumption, $Cov(X_i, X_j) = Cov(\\mu_i + \\lambda_i f + e_i, \\mu_j + \\lambda_j f + e_j)$.  \n",
    "And... stuff, we should come back to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example.  \n",
    "Consider case where we have 3 variables.\n",
    "We get a 3x3 covariance matrix $S$.\n",
    "\n",
    "Now, our analysis will have three models, for $X_1$, $X_2$, and $X_3$.\n",
    "This gives us six unkowns, namely $\\lambda_1, \\lambda_2, \\lambda_3, \\psi_1, \\psi_2, \\psi_3$.\n",
    "\n",
    "Now, our covariance matrix has the form:\n",
    "| Var($X_1$) | Cov($X_1, X_2$) | Cov($X_1, X_3$) |\n",
    "|---|---|---|\n",
    "| Cov($X_2, X_1$) | Var($X_2$) | Cov($X_2, X_3$) |\n",
    "| Cov($X_3, X_1$) | Cov($X_3, X_2$) | Var($X_3$) |\n",
    "\n",
    "By substituting the equations proved above, we get:\n",
    "| $\\lambda_1^2 + \\psi_1$ | $\\lambda_1 \\lambda_2$ | $\\lambda_1 \\lambda_3$ |\n",
    "|---|---|---|\n",
    "| $\\lambda_2 \\lambda_1$ | $\\lambda_2^2 + \\psi_2$ | $\\lambda_2 \\lambda_3$ |\n",
    "| $\\lambda_3 \\lambda_1$ | $\\lambda_3 \\lambda_2$ | $\\lambda_3^2 + \\psi_2$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these, we can extract 6 equations in 6 unkowns.  \n",
    "Actually, that was probably a tad overcomplicated.\n",
    "\n",
    "Even without the matrix, we could identify six equations:\n",
    "$Var(X_1) = \\lambda_1^2 + \\psi_1$,  \n",
    "$Var(X_2) = \\lambda_2^2 + \\psi_2$,  \n",
    "$Var(X_3) = \\lambda_3^2 + \\psi_3$,  \n",
    "$Cov(X_1, X_2) = \\lambda_1 \\lambda_2$,  \n",
    "$Cov(X_1, X_3) = \\lambda_1 \\lambda_3$,  \n",
    "$Cov(X_2, X_3) = \\lambda_2 \\lambda_3$.  \n",
    "\n",
    "This is six equations in six unknowns, since we can compute the left-hand sides.\n",
    "These can be solved, though it's not quite a linear system, and I don't see an obvious way to linearize it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we only had two observed variables.\n",
    "Then our covariance matrix would be 2x2, with just 3 unique elements.  \n",
    "However, because we have two variables, we have four unknowns.\n",
    "Then the problem is underidentified, and there are infinite solutions.  \n",
    "\n",
    "On the other hand, with a 4x4 matrix, we have 10 unique values and just 8 unknowns.\n",
    "Then we have an overidentified problem, which is actually preferable.\n",
    "We can do some form of best-fit solution.\n",
    "Then we can get a meaningful measure of statistical fit of the model.\n",
    "\n",
    "For a perfectly identified problem (the 3x3 case) the fit is always either 1 or undefined (if the problem has no solution).\n",
    "\n",
    "Note that in general, for $n \\times n$ matrix, we have $\\frac{n(n+1)}{2}$ unique values in covariance matrix and $2n$ unkowns.\n",
    "If we have $m$ factors, I think it's actually $n(m+1)$ unknowns, maybe? Need to think more carefully.\n",
    "\n",
    "Finally, note that for a best-fit solution, least-squares (regression) is generally not going to be desirable, I guess the fact we're using covariances and not regular-old random variables results in nondesirable statistical properties.\n",
    "There are other best-fit approaches that are better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model\n",
    "\n",
    "Um, kinda missed the justification, but given our sample correlation matrix $R$ and estimated population correlation matrix $\\hat{\\Sigma_{zz}}$, we can take $R - \\hat{\\Sigma_{zz}}$ to get the residuals of correlations.\n",
    "\n",
    "Then any positive residuals indicate the model is underpredicting the correlation, and negative residuals indicate the model is overpredicting the correlation.\n",
    "That is, a positive indicates that, based on the sample, the correlation should be higher than what the model is predicting, so there is some correlation not explained by the model.\n",
    "On the other hand, negative means there's actually less correlation than the model suggests.  \n",
    "The absolute values indicate the degree to which we are over/under-predicting.\n",
    "Residuals near zero indicate the model is a very good fit for those parts of the data.\n",
    "\n",
    "Individual residuals are interpreted in terms of the question of whether or not there is a common factor underlying a pair of variables, I think.\n",
    "If we see a lot of high-value residuals for a specific subset of variables against another subset, we might infer that another factor exists, and do an analysis with $m+1$ factors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs Covariance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PCA, we have possibility to analyze covariance or correlation matrices.\n",
    "\n",
    "For covariance, we'll have covariance matrix $S$, which is sample covariance matrix, and we'll be using $\\Sigma_{xx}$ as the population covariance matrix, with $\\hat\\Sigma_{xx}$ as our estimated population covariance matrix.\n",
    "\n",
    "For correlation, our notations will be $R$, $\\Sigma_{zz}$, and $\\hat\\Sigma_{zz}$ as the respective correlation matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis vs. PCA\n",
    "\n",
    "1. PCA explains total variance.\n",
    "    Factor analysis distinguishes between common and unique variance.  \n",
    "    The **communality** of variable $X_j = \\lambda_{j1}^2 + \\lambda_{j2}^2 + \\ldots + \\lambda_{jk}^2$.\n",
    "    I have no idea what the fuck that means, in terms of interpretation.  \n",
    "    The **uniqueness** of $X_j = \\psi_j$.\n",
    "    Also called the **specific variance**.\n",
    "2. PCA components (equivalent of factors) are functions of the indicator (observed, manifest) variables; in factor analysis, the indicator variables are functions of the factors.  \n",
    "    So, components are a composite of observed variables, which may represent a generalized concept with observed variables as facets of the concept.  \n",
    "    On the other hand, factors are implicit explainers for observed values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
