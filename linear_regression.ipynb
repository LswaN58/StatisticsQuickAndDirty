{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression gives us a best-fit line for a set of variables.\n",
    "In **bivariate** linear regression, we simply get a 2D line for two variables.\n",
    "\n",
    "General linear regression equation is $y = \\alpha + \\beta x$, where $\\alpha$ is the vector of intercepts, and $\\beta$ is the vector of slopes.\n",
    "For **bivariate** regression, these are each scalar values, and the equation has the classic form of $y = mx + b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the equation $y = \\alpha + \\beta x$ is our general form of the **regression model**.  \n",
    "For a computed best-fit line, the specific model is given by $\\hat{y} = a + bx$\n",
    "\n",
    "To compute the line, we have $b = \\frac{\\sum(x - \\overline{x})(y - \\overline{y})}{\\sum(x - \\overline{x})^2}$, where $\\overline{x}$ is the average value of $x$ and $\\overline{x}$ is the average value of $y$.  \n",
    "Then $a = \\overline{y} - b \\overline{x}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Error and Sum of Squared Error\n",
    "\n",
    "We'd like an idea of how well the model fits the data.\n",
    "This gives an idea of **prediction error**, or how far off the actual data is from our predictions.\n",
    "\n",
    "First, we call the difference between actual and predicted y-value the **residual** ($y - \\hat{y}$).\n",
    "\n",
    "To get this, we calculate the **sum of squared error (SSE)**, given by $\\text{SSE} = \\sum(y - \\hat{y})^2$\n",
    "\n",
    "Another name for SSE is $\\text{SS}_\\text{unexpected}$ or sum of squares unexplained.\n",
    "What does this mean? We first consider the total difference between the y-value and the mean of y, i.e. $y - \\overline{y}$.\n",
    "We can take the sum of squared differences from the mean, and call this the sum of squares total, $\\text{SS}_\\text{total} = \\sum (y - \\overline{y})$\n",
    "\n",
    "Then the difference betwen the predicted value and mean is called the explained error; the sum of squared differences here is then $\\text{SS}_\\text{explained} = \\sum(\\hat{y} - \\overline{y})$\n",
    "\n",
    "We have $\\text{SS}_\\text{total} = \\text{SS}_\\text{explained} + \\text{SS}_\\text{unexplained}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the strength of association\n",
    "\n",
    "We might be tempted to interpret the slope of the regression line as the strength of association.\n",
    "However, the slope is determined in large part by the units in use.\n",
    "Thus, if one variable is highly predictive of another, but the change in unit y is small relative to unit x, then we will have a small slope.  \n",
    "Instead, we must use the regression coefficient $r$ to measure how strongly related the variables are.\n",
    "\n",
    "We calculate the **correlation** coefficient $r = (\\frac{s_x}{s_y})\\beta$, where $s_x = $ standard deviation of $x$ and $s_y = $ standard deviation of $y$.  \n",
    "We get **coefficient of determination**, $r^2$, by simply squaring $r$.\n",
    "The interpretation of the **coefficient of determination** is that the value of $r^2$ is the proportion of variance explained by the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do regression, we get some sort of ANOVA-ish table, which helps us compute these things.\n",
    "\n",
    "Note, when we want to compare regression models, we don't compare slopes directly, at least not when the slopes are dealing with variables that have different units.\n",
    "Instead, we can compare two variables' strength of association with the outcome by looking at correlation values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
