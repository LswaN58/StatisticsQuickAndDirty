{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of PCA is to get as much information out of as few variables as possible.\n",
    "The SVD gives us orthogonal **eigenvectors**, and that orthogonality is desirable.\n",
    "Because they are orthogonal, they do not have correlations, and therefore do not represent \"redundant\" information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, we'll use the correlation matrices.\n",
    "Note, PCA/Factor analysis/whatever only need the correlation matrices, not the raw data.\n",
    "They'll always have the same set of eigenvalues and eigenvectors in use.\n",
    "\n",
    "The eigenvectors are the components, and eigenvalues are \"importances\" of the components, I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from a principal component analysis is a set of $r$ components $PC$, where each $PC$ is a weighted sum of the original variables.\n",
    "\n",
    "Suppose we originally have $p$ variables as columns of a matrix $X$.\n",
    "Then variables are $X_1, X_2, \\ldots, X_p$.\n",
    "So the principle components are given by:  \n",
    "$PC_{(1)} = w_{(1)1} X_1 + w_{(1)2} X_2 + \\ldots + w_{(1)p} X_p$  \n",
    "$PC_{(2)} = w_{(2)1} X_1 + w_{(2)2} X_2 + \\ldots + w_{(2)p} X_p$  \n",
    "$\\ldots$  \n",
    "$PC_{(r)} = w_{(r)1} X_1 + w_{(r)2} X_2 + \\ldots + w_{(r)p} X_p$  \n",
    "\n",
    "More generally, the component $PC_(j) = \\sum\\limits_{i=1}^p w_{(j)i} X_i$, where the matrix $w$ is the collection of weights.\n",
    "\n",
    "Note, the weights are the individual elements of a corresponding eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, we don't worry about whether variance is shared among variables, we just explain variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how many components we end up using, the components themselves do not change.\n",
    "Thus, there's no harm in looking at all components before choosing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Principal Components\n",
    "\n",
    "When we obtain our principal components, we can look at the weights to interpret what it means when an individual has a high value on a given component.\n",
    "\n",
    "For example, suppose we have 6 columns, and each column represents a score on a test.\n",
    "Then if we have a component where all weights are positive (e.g. weight vector is $.40, .41, .31, .44, .45, .41$), we could interpret the component as indicating an individual with high scores on all tests.\n",
    "That is, an individual with a high value for that component must have had high scores across their test scores.\n",
    "If a few components were positive and a few were negative, then an individual with a high score must have high scores in the positively-weighted tests, and poor scores on the negatively-weighted tests.\n",
    "\n",
    "Note that we generally would assume that the values are standardized, so the X-variables are assumed to be bewteen -1 and 1, with mean of 0.\n",
    "Then what we'd expect above is that an individual with a high value on the second component would, on the variables with negative weight, have high-magnitude negative values (which presumably corresponds to poor test scores, I'm still working that out in my head).\n",
    "\n",
    "Now, consider a component where some components have weights near zero.\n",
    "Then our interpretation for those variables is that they have little effect on the component value.\n",
    "\n",
    "When interpreting, always be sure to keep in mind that all components are mutually orthogonal.\n",
    "There's no way to argue that two components are correlated or anything like that.\n",
    "\n",
    "Also, note that the eigenvalue is how we interpret the importance of the given component.\n",
    "When deciding which components are worth including, we look at the eigenvalue, not the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Principal Components\n",
    "\n",
    "There's a few approaches to choosing which components to use.\n",
    "1. Make a Scree Plot, which graphs Eigenvalue on the y-axis and the component number on the x-axis. \n",
    "    We then look for a \"knee\" in the graph, i.e. where the slope goes from large in magnitude to small in magnitude.  \n",
    "    I suppose this would correspond to a point with high positive second derivative?  \n",
    "    Ooh, or maybe you could base it on an angle threshold?\n",
    "2. Only keep components whose eigenvalues are $> 1$, i.e. components with more explanatory power than the original variables.\n",
    "    This is used when analyzing correlation matrix.\n",
    "3. Set a variance-accounted-for threshold. In other words, take only as many components as needed to account for, say, 80% of variance.\n",
    "    Because the eigenvalues are the variances of the components, we can use them to determine how much overall variance has been accounted for by the components.\n",
    "    Given $n$ variables, the eigenvalues will sum to $n$.\n",
    "    To take components up to a threshold, say, $T$, we find the smallest set of eigenvalues $E$ such that $\\sum\\limits_{e \\in E} \\frac{e}{n} \\ge T$.  \n",
    "    Note that this works only because the components are mutually orthogonal.\n",
    "    Then the variance on each is completely separated from variance in other components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Loading\n",
    "\n",
    "The \"loading\" on a variable is kind of like the correlation between the variable and a given component.\n",
    "So, given a component $PC_{(i)}$ and a variable $X_j$, then the loading of variable $X_j$ on component $i$ is $r(\\text{PC}_{(i)}, X_j) = w_{(i)j} \\cdot \\sqrt(\\lambda_i)$.  \n",
    "These are sometimes preferable when interpreting, since we could have a low-eigenvalue component which has high weight on a variable but low overall correlation.\n",
    "Then looking at the raw weight might tempt us to interpret as the component that is high for that variable... but really we're dealing with leftovers, in some sense.\n",
    "So by \"weighting\" our weights by eigenvalue to get correlation, we see the strength of the relation between the variables and the component, not just how to compute component value from variable values.\n",
    "\n",
    "We can also square the loadings and interpret as the amount of variance shared between the variable and the component.\n",
    "\n",
    "Note that SPSS likes to give the loadings, rather than the eigenvalues, I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Component Scores\n",
    "\n",
    "We noted previously that we can calculate our SVD (and thus, components), from the correlation matrix, rather than from the whole original matrix.\n",
    "However, if we want to find the component scores for individuals in the original matrix, we now need the original matrix.\n",
    "There's no way to get individual component scores based on the correlations; we need to know what the original individual scores were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses for Components/Factorizations\n",
    "\n",
    "- If we have many variables, and many are correlated/colinear/whatever, we can use PCA to reduce the number of variables to use in another analysis.  \n",
    "    For example, we may want to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
